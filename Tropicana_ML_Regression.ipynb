{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tbeucler/2024_TROPICANA_ML/blob/main/Tropicana_ML_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <center> ðŸŒ´ Tropicana 2024 ðŸŒ´ <br> ML Regression for Short-Range Tropical Cyclone Forecasting ðŸŒ€\n",
        "\n",
        "*Co-authors (in alphabetical order): Tom Beucler, Milton Gomez (UNIL), Marie McGraw (CIRA)*\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1WG0jBktEJFAfOXpxRAFXtQNy0GFtsHiR\" width=75% title=\"Image of Dorian (2019), image from NASA\"></img>\n",
        "</center>\n",
        "\n",
        "Hello and welcome to this tutorial, in which you will be learning how to design linear models and random forests to forecast tropical cyclone maximum wind speed and minimum sea level pressure.  \n",
        "\n",
        "In order to get everything working we need to get access to some data.\n",
        "\n",
        "Please begin by opening this <a href=\"https://drive.google.com/drive/folders/1imMvOdZrfKPvYeO4M6s0UlMPYkbEkCj-\" target=_blank> google drive link </a> (it should open in a new tab). You'll need to add a shortcut to it on your own google drive account.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=10Y3jhNAvmBlBu7fu-b-3-oY_9uD2BY4z\" width=75% title=\"Find the add shortcut command\"></img>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1pTE5ZB9tCTLvFOC_LM5aQuIYcTUnH4WA\" width=50% title=\"Add shortcut to home in drive\"></img>\n",
        "</center>\n",
        "\n",
        "You should now have access to a hosted copy of IBTrACS, a set of netCDFs based on the SHIPS developmental dataset, and training materials kindly provided by <a href=\"https://marie-mcgraw.github.io/\">Marie McGraw</a>. We'll be loading these files into our workspace in a cell below ðŸ˜„"
      ],
      "metadata": {
        "id": "ySnS147cr_lv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prerequisites\n",
        "Let's go ahead and do some prep work for the environment by running the cell below."
      ],
      "metadata": {
        "id": "YMAEomml4PMt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D51CkJbl3KUG"
      },
      "outputs": [],
      "source": [
        "#@markdown  Run this cell to pip install some packages we'll be needing and do some additional setup. Double click it if you want to check out the source, and double click this text again to hide it :)\n",
        "#Note that %%capture allows us to supress cell output\n",
        "# %%capture\n",
        "!pip install tcbenchmark==0.0.3rc12\n",
        "!pip install cartopy\n",
        "!pip install memory_profiler\n",
        "!pip install metpy\n",
        "!pip install dask-ml\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import os\n",
        "import xarray as xr\n",
        "\n",
        "# To make this notebook's output stable across runs\n",
        "rnd_seed = 42\n",
        "rnd_gen = np.random.default_rng(rnd_seed)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation"
      ],
      "metadata": {
        "id": "AALEuVs54ElY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start out by accessing our Google Drive in the notebook. You'll get a popup asking you to authorize the access. It looks like this:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1YlGmAYLjcGGdJbRgXO_uw5U2BIekxggT\" width=55% title=\"Authorize access to Google Drive command\"></img>\n",
        "</center>"
      ],
      "metadata": {
        "id": "QxYNjHXA3hgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/') # This will mount your entire drive! Unfortunately, you can't mount just a folder."
      ],
      "metadata": {
        "id": "TjE3wAK43qXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you, like us, made the shortcut in your base directory in Google Drive, you should be able to list the folders by using the following code:\n",
        "print(os.listdir('/content/drive/MyDrive/Tropicana_ML_data'))\n",
        "# ['SHIPS_netcdfs', 'Marie_McGraw', 'IBTrACS'] should be printed to the cell output.\n",
        "\n",
        "# And with the follow bash command, we can look at some files in the SHIPS_netcdfs folder.\n",
        "!ls /content/drive/MyDrive/Tropicana_ML_data/SHIPS_netcdfs/ -lah | grep 052019\n",
        "\n",
        "# And this will be the path to the folder where the IBTrACS .csv is located\n",
        "!echo /content/drive/MyDrive/Tropicana_ML_data/IBTrACS/"
      ],
      "metadata": {
        "id": "XbGDWOS-DRCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have access to google drive, we have the data that we want to work with. Let's take a moment to load a sample and see what kind of information we have."
      ],
      "metadata": {
        "id": "HQNuXw94C-wG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" --------------------------------------------------------------------------\n",
        "We already imported xarray, numpy, pandas, and matplotlib in the setup cell, so\n",
        "we can just go ahead and open up the file we're interested in. For this example\n",
        "we've chosen Dorian (2019) and Hagibis (2019)\n",
        "\"\"\";\n",
        "dorian_data = xr.open_dataset('/content/drive/MyDrive/Tropicana_ML_data/SHIPS_netcdfs/AL052019.nc')\n",
        "hagibis_data = xr.open_dataset('/content/drive/MyDrive/Tropicana_ML_data/SHIPS_netcdfs/WP202019.nc')"
      ],
      "metadata": {
        "id": "3wAJBBf1vqd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's get an idea of what data variables we have for these storms:\n",
        "print(f'We have {len(dorian_data.data_vars)} Dorian Datavars: \\n',\n",
        "      *dorian_data.data_vars,\n",
        "      f'\\n We have {len(hagibis_data.data_vars)} Hagibis Datavars: \\n',\n",
        "      *hagibis_data.data_vars,\n",
        "      sep=' ')\n",
        "\n",
        "# This tracks with what we know: SHIPS has a different set of variables for different basins"
      ],
      "metadata": {
        "id": "l06KFiz8JtiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, we're interested in looking at seasons of storms and not just individual storms. We're developing a benchmark dataset + framework for tropical cyclone studies called <a href='https://github.com/msgomez06/TCBench_Alpa'> TCBench</a>, so we'll go ahead and rely on it to look at storm seasons."
      ],
      "metadata": {
        "id": "2UFFq9eRT6s2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tcbenchmark.utils import toolbox"
      ],
      "metadata": {
        "id": "4gPVX7PZ3Mfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Please define the seasons you would like to study in this notebook HERE\n",
        "season_list = [*range(2010,2020),] # The default is to load 10 seasons, from 2010 - 2019, for this exercise."
      ],
      "metadata": {
        "id": "tr09x-pScGcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tracks = toolbox.get_TC_seasons(\n",
        "    tracks_path='/content/drive/MyDrive/Tropicana_ML_data/IBTrACS/',\n",
        "    season_list=season_list,\n",
        "    datadir_path = '/content/drive/MyDrive/Tropicana_ML_data/')"
      ],
      "metadata": {
        "id": "fGQw2vuu3TWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using TCBench, each storm is loaded as a <i>track</i> object. The `get_TC_seasons` function will return a dictionary with the seasons as keys, and each item will be a list of track objects for the storms that are recorded in IBTrACS for that season."
      ],
      "metadata": {
        "id": "kr-8-IJKVn7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's take a single storm as a sample from the 2019 season\n",
        "test_storm = rnd_gen.choice(tracks[2019])"
      ],
      "metadata": {
        "id": "bTe01tKP1K_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If it's the first time you ran the code cell above, you should have Desmond stored in test_storm. Let's start by looking at some of its attributes\n",
        "print(\n",
        "    test_storm.uid, # By default when loading IBTrACS, TCBench will use the SID as the unique identifier\n",
        "    test_storm.ALT_ID, # and the ATCF ID, which is used by SHIPS, will be stored in the ALT_ID field.\n",
        "    test_storm.name, # The name (or lack thereof) is stored in name\n",
        "    test_storm.wind, # This will return the US agency wind timeseries\n",
        "    test_storm.pressure, # And this the US agency pressure timeseries\n",
        "    sep = \"\\n-------\\n\"\n",
        ")"
      ],
      "metadata": {
        "id": "d5WATtny20Lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are some wind and pressure values missing in the tracks for Desmond! This would present a problem if we tried to feed it directly into an algorithm. However, TCBench can help us retrive the data we're interested while taking care of those kinds of issues.\n",
        "\n",
        "First, because we know we'll be working with SHIPS (and therefore need the ATCF ID), let's verify that the storms we're working with have a valid ATCF ID."
      ],
      "metadata": {
        "id": "zTaxEqAX74SG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Some storms may not have an associated ACTF ID (e.g., unnamed storms that were not tracked by a US agency). Let's check if we have any of those in 2019\n",
        "for track in tracks[2019]:\n",
        "    if track.ALT_ID is None: print(track, track.ALT_ID)"
      ],
      "metadata": {
        "id": "wFa075KwXE8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've verified that some of the storms in IBTrACS don't have an associated ACTF ID. Let's go ahead and remove those from our lists, since we won't be able to use those for our study. We'll also limit ourselves to the North Atlantic basin to speed things up.\n",
        "\n",
        "<small>Side note: The TCTrack object currently doesn't have an easily accessible basin attribute, but if you'd like to filter by basin you could do so with the first two letters of the ATCF ID</small>"
      ],
      "metadata": {
        "id": "kUmzM8lonIfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for season, storm_list in tracks.items():\n",
        "    for storm in storm_list.copy():\n",
        "        if storm.ALT_ID is None or not ('AL' in storm.ALT_ID): storm_list.remove(storm)"
      ],
      "metadata": {
        "id": "cq0jQ6h3mwzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If we run this again we shouldn't see anything\n",
        "for track in tracks[2019]:\n",
        "    if track.ALT_ID is None: print(track, track.ALT_ID)"
      ],
      "metadata": {
        "id": "iB1iDzUUoMqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a relatively clean set of tracks, let's go ahead and define the time series that we will generate for our regression algorithm.\n",
        "\n",
        "Today, we'll be predicting the 24h intensification of a storm using a series of SHIPS predictors, but we'll limit the information to that which was available at forecasting (i.e., we won't be using any predictors indexed for lead_time>0).\n",
        "\n",
        "We'll also use 5 timesteps for each predictor.\n",
        "\n",
        "Finally, we have to come up with a definition for our training set, validation set, and test set. These will allow us, respectively, to fit our algorithm, tune our hyperparameters, and objectively evaluate model performance once we're happy with their performance on the training and validation sets.\n",
        "We'll use 60% of the seasons for training, 20% of the seasons for validation, and 20% of the seasons for testing."
      ],
      "metadata": {
        "id": "IoLjyAkQ9wK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll define the number of hours into the future we'll be predicting\n",
        "delta_target = 24\n",
        "\n",
        "# and store the minimum and maximum leadtimes (both = 0)\n",
        "leadtime_min = 0\n",
        "leadtime_max = 0\n",
        "\n",
        "# And define the number of steps for each predictor.\n",
        "timeseries_length = 5 # Values of 4 and lower currently cause a crash\n",
        "\n",
        "# Let's define the splits dictionary\n",
        "splits = {\n",
        "    'train': 0.6,\n",
        "    'validation': 0.2,\n",
        "    'test': 0.2\n",
        "}\n",
        "\n",
        "# And finally, the location for the data folder\n",
        "timeseries_dir = '/content/drive/MyDrive/Tropicana_ML_data/SHIPS_netcdfs'"
      ],
      "metadata": {
        "id": "__fSJXbqC9V-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that by default TCBench will serve the following variables for the time series\n",
        "from tcbenchmark.utils.constants import default_ships_vars\n",
        "print(default_ships_vars)\n",
        "# If you want to pass a different set of predictors from the developmental dataset, simply pass the list as the 'variables' argument when calling get_timeseries_sets below."
      ],
      "metadata": {
        "id": "brEvJXk760dK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the function get_timeseries_sets from the toolbox module\n",
        "data_dictionary = toolbox.get_timeseries_sets(\n",
        "    splits=splits,                     # splits: Dictionary specifying the split ratios for train, validation, and test sets.\n",
        "    season_dict=tracks,                # season_dict: Dictionary containing the track data for each season.\n",
        "    leadtime_min=leadtime_min,         # leadtime_min: Minimum lead time for the time series data (e.g., -12 hours).\n",
        "    leadtime_max=leadtime_max,         # leadtime_max: Maximum lead time for the time series data (e.g., 0 hours).\n",
        "    timeseries_length=timeseries_length, # timeseries_length: Length of the time series data (number of timesteps).\n",
        "    delta_target=delta_target,         # delta_target: Lead time in hours for the target intensity (e.g., 24 hours).\n",
        "    timeseries_dir=timeseries_dir,     # timeseries_dir: Directory path where the time series data files are stored.\n",
        ")"
      ],
      "metadata": {
        "id": "at8YVgof9vrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Analysis"
      ],
      "metadata": {
        "id": "krPs2hRnhAZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate and print the structure with data shapes\n",
        "for main_key in data_dictionary.keys():\n",
        "    print(f'\\n  {main_key}')\n",
        "    for sub_key in data_dictionary[main_key].keys():\n",
        "        print(f'  {sub_key}')\n",
        "        data_item = data_dictionary[main_key][sub_key]\n",
        "        # Using the fact that data_item is a list or array-like structure\n",
        "        if hasattr(data_item, 'shape'):  # For numpy arrays, pandas DataFrames, etc.\n",
        "            print(f'    Shape: {data_item.shape}')\n",
        "        else:\n",
        "            print(f'    Value: {data_item}')"
      ],
      "metadata": {
        "id": "Ad6CS4Y4Tk2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the training data from the data dictionary\n",
        "train_data = data_dictionary['train']\n",
        "\n",
        "# Calculate mean and standard deviation for each training variable\n",
        "data_means = train_data['data'].mean(axis=(0, 1))  # Mean of all data variables across the first two dimensions\n",
        "data_stds = train_data['data'].std(axis=(0, 1))  # Standard deviation of all data variables across the first two dimensions\n",
        "intensity_deltas_means = train_data['intensity_deltas'].mean(axis=0)  # Mean of intensity deltas\n",
        "intensity_deltas_stds = train_data['intensity_deltas'].std(axis=0)  # Standard deviation of intensity deltas\n",
        "base_intensities_means = train_data['base_intensities'].mean(axis=0)  # Mean of base intensities\n",
        "base_intensities_stds = train_data['base_intensities'].std(axis=0)  # Standard deviation of base intensities\n",
        "\n",
        "# Create a combined list of all variable names\n",
        "all_vars = default_ships_vars + ['Delta Intensity', 'Delta MSLP', 'Base Intensity', 'Base MSLP']\n",
        "\n",
        "# Concatenate all means and standard deviations into single arrays\n",
        "all_means = np.concatenate([data_means, intensity_deltas_means, base_intensities_means])\n",
        "all_stds = np.concatenate([data_stds, intensity_deltas_stds, base_intensities_stds])"
      ],
      "metadata": {
        "id": "aCPHTEQQYvwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot PDFs (Probability Density Functions) with statistics for each variable\n",
        "plt.figure(figsize=(20, 15))  # Set the figure size\n",
        "num_vars = len(all_vars)  # Total number of variables\n",
        "for i in range(num_vars):\n",
        "    plt.subplot(6, 4, i + 1)  # Create a subplot for each variable in a 6x4 grid\n",
        "    if i < 18:  # First 18 variables (ship data)\n",
        "        var_data = train_data['data'][:, :, i].flatten()  # Flatten the 3D array to 1D for histogram\n",
        "    elif i < 20:  # Next 2 variables (intensity deltas)\n",
        "        var_data = train_data['intensity_deltas'][:, i - 18]  # Select the intensity delta variables\n",
        "    else:  # Last 2 variables (base intensities)\n",
        "        var_data = train_data['base_intensities'][:, i - 20]  # Select the base intensity variables\n",
        "\n",
        "    plt.hist(var_data, bins=30, density=True, alpha=0.6, color='k')  # Plot the histogram with density\n",
        "    plt.title(f\"{all_vars[i]}\\nMean: {all_means[i]:.2f}, Std: {all_stds[i]:.2f}\")  # Title with variable name, mean, and std\n",
        "    plt.xlabel(all_vars[i])  # X-axis label with variable name\n",
        "    plt.ylabel('Density')  # Y-axis label as 'Density'\n",
        "\n",
        "plt.tight_layout()  # Adjust layout to prevent overlap\n",
        "plt.show()  # Display the plot"
      ],
      "metadata": {
        "id": "LuettoaHrEEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the first intensity_deltas variable (Delta Intensity)\n",
        "delta_intensity = train_data['intensity_deltas'][:, 0]\n",
        "\n",
        "# Calculate correlations between each variable at each timestep and the first intensity_deltas variable\n",
        "timesteps = train_data['data'].shape[1]\n",
        "correlations = np.zeros((18, timesteps))\n",
        "\n",
        "for var in range(18):\n",
        "    for t in range(timesteps):\n",
        "        var_data = train_data['data'][:, t, var]\n",
        "        correlations[var, t] = np.corrcoef(var_data, delta_intensity)[0, 1]\n",
        "\n",
        "# Create time labels assuming each timestep represents a 6-hour interval\n",
        "time_labels = np.flip(['t_IC', 't_IC - 6hr', 't_IC - 12hr', 't_IC - 18hr', 't_IC - 24hr'])\n",
        "# where t_IC is the initial conditions time\n",
        "# We're trying to predict the intensity at t_IC+24hr from t_IC\n",
        "\n",
        "# Create a heatmap of the correlation matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlations, annot=True, cmap='coolwarm', xticklabels=time_labels, yticklabels=default_ships_vars, center=0)\n",
        "\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Variables')\n",
        "plt.title('Pearson Correlation of Variables with Delta Intensity')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l50R1TsYiJ25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the second intensity_deltas variable (Delta MSLP)\n",
        "delta_mslp = train_data['intensity_deltas'][:, 1]\n",
        "\n",
        "# Calculate correlations between each variable at each timestep and Delta MSLP\n",
        "timesteps = train_data['data'].shape[1]\n",
        "correlations = np.zeros((18, timesteps))\n",
        "\n",
        "for var in range(18):\n",
        "    for t in range(timesteps):\n",
        "        var_data = train_data['data'][:, t, var]\n",
        "        correlations[var, t] = np.corrcoef(var_data, delta_mslp)[0, 1]\n",
        "\n",
        "# Create time labels assuming each timestep represents a 6-hour interval\n",
        "time_labels = np.flip(['t_IC', 't_IC - 6hr', 't_IC - 12hr', 't_IC - 18hr', 't_IC - 24hr'])\n",
        "# where t_IC is the initial conditions time\n",
        "# We're trying to predict the intensity at t_IC+24hr from t_IC\n",
        "\n",
        "# Create a heatmap of the correlation matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlations, annot=True, cmap='coolwarm',\n",
        "            xticklabels=time_labels, yticklabels=default_ships_vars,\n",
        "            center=0)\n",
        "\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Variables')\n",
        "plt.title('Pearson Correlation of Variables with Delta Mean Sea Level Pressure')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1-zsVhupkewW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Pre-processing"
      ],
      "metadata": {
        "id": "l7S4Ptnxpivn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For a 24-hour lead time prediction of Delta (Intensity, MSLP) without memory"
      ],
      "metadata": {
        "id": "6lm1o-o1pk2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_data(X, mean, std):\n",
        "    \"\"\"\n",
        "    Normalize the data using the provided mean and standard deviation.\n",
        "\n",
        "    Parameters:\n",
        "    - X: input data to be normalized\n",
        "    - mean: mean for normalization\n",
        "    - std: standard deviation for normalization\n",
        "\n",
        "    Returns:\n",
        "    - X_normalized: normalized data\n",
        "    \"\"\"\n",
        "    return (X - mean) / std\n",
        "\n",
        "def denormalize_data(X_normalized, mean, std):\n",
        "    \"\"\"\n",
        "    Denormalize the data using the provided mean and standard deviation.\n",
        "\n",
        "    Parameters:\n",
        "    - X_normalized: normalized data to be denormalized\n",
        "    - mean: mean for denormalization\n",
        "    - std: standard deviation for denormalization\n",
        "\n",
        "    Returns:\n",
        "    - X: denormalized data\n",
        "    \"\"\"\n",
        "    return X_normalized * std + mean"
      ],
      "metadata": {
        "id": "2WgfLxlPw-J8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the training, validation, and test data from the data dictionary\n",
        "train_data = data_dictionary['train']\n",
        "val_data = data_dictionary['validation']\n",
        "test_data = data_dictionary['test']\n",
        "\n",
        "# Extract X and Y for training\n",
        "X_train = train_data['data'][:, 0, 1:]  # Use only the first timestep (column 0) for all samples, and eliminate the first variable (DELV)\n",
        "Y_train = train_data['intensity_deltas']  # Use both Delta Intensity and Delta MSLP\n",
        "\n",
        "# Normalize X and Y for training\n",
        "X_train_normalized = normalize_data(X_train, data_means[1:], data_stds[1:])\n",
        "Y_train_normalized = normalize_data(Y_train, intensity_deltas_means, intensity_deltas_stds)\n",
        "\n",
        "# Extract and normalize X and Y for validation\n",
        "X_val = val_data['data'][:, 0, 1:]\n",
        "Y_val = val_data['intensity_deltas']\n",
        "X_val_normalized = normalize_data(X_val, data_means[1:], data_stds[1:])\n",
        "Y_val_normalized = normalize_data(Y_val, intensity_deltas_means, intensity_deltas_stds)\n",
        "\n",
        "# Extract and normalize X and Y for test\n",
        "X_test = test_data['data'][:, 0, 1:]\n",
        "Y_test = test_data['intensity_deltas']\n",
        "X_test_normalized = normalize_data(X_test, data_means[1:], data_stds[1:])\n",
        "Y_test_normalized = normalize_data(Y_test, intensity_deltas_means, intensity_deltas_stds)\n",
        "\n",
        "# Create metadata\n",
        "X_metadata = default_ships_vars[1:]  # Exclude the first variable (DELV)\n",
        "Y_metadata = ['Delta Intensity', 'Delta MSLP']"
      ],
      "metadata": {
        "id": "fmRstVrEprO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For a 24-hour lead time prediction of Delta (Intensity, MSLP) with memory"
      ],
      "metadata": {
        "id": "_mSu48JfxswE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vFLHzN-Wxy-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For an auto-regressive model"
      ],
      "metadata": {
        "id": "_UCKqfJLqKK5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Coming soon. You may ask [Chia-Ying Lee](https://journals.ametsoc.org/view/journals/clim/29/21/jcli-d-15-0909.1.xml?tab_body=pdf) for advice ðŸ˜ƒ"
      ],
      "metadata": {
        "id": "5xLb_Zz6qL-B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Machine Learning Model Hierarchy"
      ],
      "metadata": {
        "id": "Fks9nV2H1e-J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we develop a model hierarchy targeting the 24-hour intensification rate. These models will serve as baselines for comparison. For each model, we will keep track of the error statistics on the training, validation, and test sets, as well as the number of parameters and the nature of the model throughout the process."
      ],
      "metadata": {
        "id": "iZw2PrroS149"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty dictionary to accumulate all model scores\n",
        "model_scores = {}"
      ],
      "metadata": {
        "id": "kcgo4D_Iv58p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Run to install ML libraries\n",
        "%%capture\n",
        "!pip install xgboost\n",
        "!pip install adjustText"
      ],
      "metadata": {
        "cellView": "form",
        "id": "8-lDRHE-NLs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Run to import ML libraries\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "from sklearn.base import clone, BaseEstimator, RegressorMixin\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV, KFold, PredefinedSplit, RandomizedSearchCV\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "from adjustText import adjust_text"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1kCsesdgri_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scoring functions"
      ],
      "metadata": {
        "id": "jj9r0HQXS6-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def score_model(model, X_train, Y_train, X_val=None, Y_val=None, X_test=None, Y_test=None, model_name=\"model\", num_params=None):\n",
        "    \"\"\"\n",
        "    Function to score the model and accumulate the scores in a dictionary for training, validation, and test sets.\n",
        "\n",
        "    Parameters:\n",
        "    - model: the trained regression model\n",
        "    - X_train: the input features for training (normalized)\n",
        "    - Y_train: the target values for training (normalized)\n",
        "    - X_val: the input features for validation (optional, normalized)\n",
        "    - Y_val: the target values for validation (optional, normalized)\n",
        "    - X_test: the input features for test (optional, normalized)\n",
        "    - Y_test: the target values for test (optional, normalized)\n",
        "    - model_name: a name for the model (default is \"model\")\n",
        "    - num_params: the number of trainable parameters (optional)\n",
        "\n",
        "    Accumulates:\n",
        "    - model_scores: a dictionary containing RMSE, MAE, RÂ² per variable, and number of parameters for each model on training, validation, and test sets\n",
        "    \"\"\"\n",
        "    global model_scores\n",
        "\n",
        "    def calculate_scores(X, Y):\n",
        "        \"\"\"\n",
        "        Helper function to calculate RMSE, MAE, and RÂ² scores.\n",
        "        \"\"\"\n",
        "        Y_pred = model.predict(X)\n",
        "        Y_denorm = denormalize_data(Y, intensity_deltas_means, intensity_deltas_stds)\n",
        "        Y_pred_denorm = denormalize_data(Y_pred, intensity_deltas_means, intensity_deltas_stds)\n",
        "\n",
        "        rmse_knots = np.sqrt(mean_squared_error(Y_denorm[:, 0], Y_pred_denorm[:, 0]))\n",
        "        mae_knots = mean_absolute_error(Y_denorm[:, 0], Y_pred_denorm[:, 0])\n",
        "        rmse_hPa = np.sqrt(mean_squared_error(Y_denorm[:, 1], Y_pred_denorm[:, 1]))\n",
        "        mae_hPa = mean_absolute_error(Y_denorm[:, 1], Y_pred_denorm[:, 1])\n",
        "        r2_wind = r2_score(Y[:, 0], Y_pred[:, 0])\n",
        "        r2_pressure = r2_score(Y[:, 1], Y_pred[:, 1])\n",
        "        return rmse_knots, mae_knots, rmse_hPa, mae_hPa, r2_wind, r2_pressure\n",
        "\n",
        "    # Initialize score dictionary for the current model\n",
        "    score_dict = {}\n",
        "\n",
        "    # Calculate and store scores for the training set\n",
        "    train_rmse_knots, train_mae_knots, train_rmse_hPa, train_mae_hPa, train_r2_wind, train_r2_pressure = calculate_scores(X_train, Y_train)\n",
        "    score_dict['train'] = {\n",
        "        \"RMSE (knots)\": train_rmse_knots,\n",
        "        \"MAE (knots)\": train_mae_knots,\n",
        "        \"RMSE (hPa)\": train_rmse_hPa,\n",
        "        \"MAE (hPa)\": train_mae_hPa,\n",
        "        \"RÂ² wind\": train_r2_wind,\n",
        "        \"RÂ² pressure\": train_r2_pressure\n",
        "    }\n",
        "\n",
        "    # Calculate and store scores for the validation set (if provided)\n",
        "    if X_val is not None and Y_val is not None:\n",
        "        val_rmse_knots, val_mae_knots, val_rmse_hPa, val_mae_hPa, val_r2_wind, val_r2_pressure = calculate_scores(X_val, Y_val)\n",
        "        score_dict['validation'] = {\n",
        "            \"RMSE (knots)\": val_rmse_knots,\n",
        "            \"MAE (knots)\": val_mae_knots,\n",
        "            \"RMSE (hPa)\": val_rmse_hPa,\n",
        "            \"MAE (hPa)\": val_mae_hPa,\n",
        "            \"RÂ² wind\": val_r2_wind,\n",
        "            \"RÂ² pressure\": val_r2_pressure\n",
        "        }\n",
        "\n",
        "    # Calculate and store scores for the test set (if provided)\n",
        "    if X_test is not None and Y_test is not None:\n",
        "        test_rmse_knots, test_mae_knots, test_rmse_hPa, test_mae_hPa, test_r2_wind, test_r2_pressure = calculate_scores(X_test, Y_test)\n",
        "        score_dict['test'] = {\n",
        "            \"RMSE (knots)\": test_rmse_knots,\n",
        "            \"MAE (knots)\": test_mae_knots,\n",
        "            \"RMSE (hPa)\": test_rmse_hPa,\n",
        "            \"MAE (hPa)\": test_mae_hPa,\n",
        "            \"RÂ² wind\": test_r2_wind,\n",
        "            \"RÂ² pressure\": test_r2_pressure\n",
        "        }\n",
        "\n",
        "    # Get the number of trainable parameters\n",
        "    if num_params is None:\n",
        "        if isinstance(model, LinearRegression):\n",
        "            num_params = model.coef_.size + model.intercept_.size\n",
        "        elif isinstance(model, RandomForestRegressor):\n",
        "            num_params = sum(estimator.tree_.node_count for estimator in model.estimators_)\n",
        "        elif isinstance(model, xgb.XGBRegressor):\n",
        "            # Careful: This is a heuristic...\n",
        "            n_estimators = model.get_params()['n_estimators']\n",
        "            max_depth = model.get_params()['max_depth']\n",
        "            # Estimate the number of nodes per tree\n",
        "            nodes_per_tree = 2 ** max_depth - 1\n",
        "            # Total number of nodes in the model\n",
        "            num_params = n_estimators * nodes_per_tree\n",
        "        else:\n",
        "            num_params = \"N/A\"  # For other models, we might not have a straightforward way to calculate parameters\n",
        "\n",
        "\n",
        "    # Add the number of parameters to the score dictionary\n",
        "    score_dict['num_params'] = num_params\n",
        "\n",
        "    # Accumulate the scores in the global model_scores dictionary\n",
        "    model_scores[model_name] = score_dict\n",
        "\n",
        "# Print the accumulated model scores in a readable way\n",
        "def print_model_scores(model_scores):\n",
        "    for model_name, scores in model_scores.items():\n",
        "        print(f\"Model: {model_name}\")\n",
        "        for dataset, metrics in scores.items():\n",
        "            if dataset != 'num_params':\n",
        "                print(f\"  {dataset.capitalize()}:\")\n",
        "                for metric, value in metrics.items():\n",
        "                    print(f\"    {metric}: {value:.4f}\")\n",
        "            else:\n",
        "                print(f\"  Number of Parameters: {metrics}\")\n",
        "        print()"
      ],
      "metadata": {
        "id": "xXKV82SI6eB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multiple Linear Regression\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1JD1qaj7J077udgMwCQdkvNPPqlNtUzNd\" width=15% title=\"Epicycloid image by the Mathematics of Waves and Materials Group @ the University of Manchester\"></img> <img src=\"https://drive.google.com/uc?export=view&id=1rjXW5I8mxoAFVd7pmtySHBGoDUfpuWIo\" width=15% title=\"Epicycloid image by the Mathematics of Waves and Materials Group @ the University of Manchester\"></img> <img src=\"https://drive.google.com/uc?export=view&id=1JD1qaj7J077udgMwCQdkvNPPqlNtUzNd\" width=15% title=\"Epicycloid image by the Mathematics of Waves and Materials Group @ the University of Manchester\"></img> <img src=\"https://drive.google.com/uc?export=view&id=1rjXW5I8mxoAFVd7pmtySHBGoDUfpuWIo\" width=15% title=\"Epicycloid image by the Mathematics of Waves and Materials Group @ the University of Manchester\"></img> <img src=\"https://drive.google.com/uc?export=view&id=1JD1qaj7J077udgMwCQdkvNPPqlNtUzNd\" width=15% title=\"Epicycloid image by the Mathematics of Waves and Materials Group @ the University of Manchester\"></img>\n",
        "\n",
        "</center>"
      ],
      "metadata": {
        "id": "Q_OQdfk1QZqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train_normalized, Y_train_normalized)\n",
        "\n",
        "# Score the model on training, validation, and test sets\n",
        "score_model(model, X_train_normalized, Y_train_normalized, X_val_normalized, Y_val_normalized, X_test_normalized, Y_test_normalized, model_name=\"Linear Regression\")\n",
        "\n",
        "# Print the accumulated scores dictionary\n",
        "print_model_scores(model_scores)"
      ],
      "metadata": {
        "id": "H1zt5SUVTdMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For comparison, score the persistence model\n",
        "class Persistence(BaseEstimator, RegressorMixin):\n",
        "    \"\"\"\n",
        "    A persistence model that predicts zero for any input in the denormalized space.\n",
        "    \"\"\"\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        return self  # Nothing to do here as predictions are constant.\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Calculate the normalized value that corresponds to zero in the denormalized space\n",
        "        return -self.mean / self.std * np.ones((X.shape[0], len(self.mean)))"
      ],
      "metadata": {
        "id": "zMO-HoZ9QdvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the persistence model using the means and stds for Y\n",
        "persistence_model = Persistence(mean=intensity_deltas_means, std=intensity_deltas_stds)\n",
        "\n",
        "# Fit the persistence model (though it does nothing)\n",
        "persistence_model.fit(X_train_normalized, Y_train_normalized)\n",
        "\n",
        "# Score the persistence model on training, validation, and test sets\n",
        "score_model(persistence_model, X_train_normalized, Y_train_normalized,\n",
        "            X_val_normalized, Y_val_normalized, X_test_normalized,\n",
        "            Y_test_normalized, model_name=\"Persistence\", num_params=1)\n",
        "# num_params = 1 to avoid singularity\n",
        "\n",
        "# Print the accumulated scores dictionary\n",
        "print_model_scores(model_scores)"
      ],
      "metadata": {
        "id": "gdlS7mWxRjvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is not great and it even looks like we're overfitting a bit! Can we do better by only selecting the best features?"
      ],
      "metadata": {
        "id": "408UNVuHxOOj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forward Sequential Feature Selection\n",
        "\n",
        "Forward feature sequential selection is a greedy algorithm that adds one input at a time based on a cross-validated score until the addition of further inputs does not improve the model's performance.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1tGCD3ALPAp_a-Fjgrnx9dIgaxHruEEhC\" width=75% title=\"Feature Selection, src: Adobe Stock\"></img>\n",
        "</center>"
      ],
      "metadata": {
        "id": "TSZrlarCzFO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_feature_selection(X_train, Y_train, X_val, Y_val, feature_names, objective, model_class=LinearRegression):\n",
        "    \"\"\"\n",
        "    Perform forward sequential feature selection based on a custom objective.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train: the input features for training (normalized)\n",
        "    - Y_train: the target values for training (normalized)\n",
        "    - X_val: the input features for validation (normalized)\n",
        "    - Y_val: the target values for validation (normalized)\n",
        "    - feature_names: names of the features\n",
        "    - objective: function to optimize during feature selection, should return a score\n",
        "    - model_class: the regression model class to be used (default is LinearRegression)\n",
        "\n",
        "    Returns:\n",
        "    - best_features: indices of the best selected features\n",
        "    - best_model: the best trained model\n",
        "    \"\"\"\n",
        "    n_features = X_train.shape[1]  # Total number of features\n",
        "    selected_features = []  # List to hold the indices of selected features\n",
        "    remaining_features = list(range(n_features))  # List of indices of features not yet selected\n",
        "    best_score = float('-inf')  # Initialize the best score to negative infinity\n",
        "    best_model = None  # Placeholder for the best model\n",
        "    best_features = []  # Placeholder for the indices of the best features\n",
        "\n",
        "    while remaining_features:\n",
        "        current_best_score = float('-inf')  # Best score for the current iteration\n",
        "        current_best_feature = None  # Best feature for the current iteration\n",
        "        current_best_model = None  # Best model for the current iteration\n",
        "\n",
        "        # Evaluate each remaining feature to find the best one to add\n",
        "        for feature in remaining_features:\n",
        "            features_to_try = selected_features + [feature]  # Current set of features to try\n",
        "            model = model_class()  # Instantiate the model\n",
        "            model.fit(X_train[:, features_to_try], Y_train)  # Fit the model on the training data\n",
        "            score = objective(model, X_val[:, features_to_try], Y_val)  # Evaluate the model using the objective function\n",
        "\n",
        "            # Update the current best feature, score, and model if the score improves\n",
        "            if score > current_best_score:\n",
        "                current_best_score = score\n",
        "                current_best_feature = feature\n",
        "                current_best_model = clone(model)\n",
        "\n",
        "        # Add the best feature of the current iteration to the selected features\n",
        "        selected_features.append(current_best_feature)\n",
        "        remaining_features.remove(current_best_feature)\n",
        "\n",
        "        # Update the overall best score, model, and features if the current score is better\n",
        "        if current_best_score > best_score:\n",
        "            best_score = current_best_score\n",
        "            best_model = current_best_model\n",
        "            best_features = selected_features.copy()\n",
        "\n",
        "        # Fit the best model with the current best features\n",
        "        best_model = model_class()\n",
        "        best_model.fit(X_train[:, selected_features], Y_train)\n",
        "        best_score = current_best_score\n",
        "\n",
        "    return best_features, best_model"
      ],
      "metadata": {
        "id": "eSjjhimVziGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to see whether the best linear model to predict wind is the same as the best model to predict pressure."
      ],
      "metadata": {
        "id": "tYXcDKUD12tJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define custom objective functions\n",
        "def r2_wind_objective(model, X, Y):\n",
        "    Y_pred = model.predict(X)\n",
        "    return r2_score(Y[:, 0], Y_pred[:, 0])\n",
        "\n",
        "def r2_pressure_objective(model, X, Y):\n",
        "    Y_pred = model.predict(X)\n",
        "    return r2_score(Y[:, 1], Y_pred[:, 1])"
      ],
      "metadata": {
        "id": "vCSC-fJ72GI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform forward sequential feature selection for wind and pressure\n",
        "feature_names = default_ships_vars[1:]  # Exclude the first variable (DELV)\n",
        "\n",
        "best_features_wind, best_model_wind = forward_feature_selection(\n",
        "    X_train_normalized, Y_train_normalized, X_val_normalized, Y_val_normalized,\n",
        "    feature_names, r2_wind_objective, model_class=LinearRegression\n",
        ")\n",
        "\n",
        "best_features_pressure, best_model_pressure = forward_feature_selection(\n",
        "    X_train_normalized, Y_train_normalized, X_val_normalized, Y_val_normalized,\n",
        "    feature_names, r2_pressure_objective, model_class=LinearRegression\n",
        ")\n",
        "\n",
        "# Print the selected features for wind and pressure\n",
        "print(\"Selected feature indices for wind:\", best_features_wind)\n",
        "print(\"Selected feature names for wind:\", [feature_names[i] for i in best_features_wind])\n",
        "print(\"Selected feature indices for pressure:\", best_features_pressure)\n",
        "print(\"Selected feature names for pressure:\", [feature_names[i] for i in best_features_pressure])"
      ],
      "metadata": {
        "id": "2ICLu27K2I2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Refit the best models with the selected features\n",
        "best_model_wind.fit(X_train_normalized[:, best_features_wind], Y_train_normalized)\n",
        "best_model_pressure.fit(X_train_normalized[:, best_features_pressure], Y_train_normalized)\n",
        "\n",
        "# Calculate and print scores for the best models, including the test set\n",
        "score_model(best_model_wind,\n",
        "            X_train_normalized[:, best_features_wind], Y_train_normalized,\n",
        "            X_val_normalized[:, best_features_wind], Y_val_normalized,\n",
        "            X_test_normalized[:, best_features_wind], Y_test_normalized,\n",
        "            model_name=\"SFS Linear Regression (Wind)\")\n",
        "\n",
        "score_model(best_model_pressure,\n",
        "            X_train_normalized[:, best_features_pressure], Y_train_normalized,\n",
        "            X_val_normalized[:, best_features_pressure], Y_val_normalized,\n",
        "            X_test_normalized[:, best_features_pressure], Y_test_normalized,\n",
        "            model_name=\"SFS Linear Regression (Pressure)\")\n",
        "\n",
        "print_model_scores(model_scores)"
      ],
      "metadata": {
        "id": "YxS38epn2QDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This works! We avoided overfitting our linear regression model by strategically selecting our features ðŸ˜€\n",
        "\n",
        "Now let's see if we can do the same with a random forest ðŸŒ²"
      ],
      "metadata": {
        "id": "6T-5OPF0-Jf-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1NSvebXiF9yW_PRoFKMfFRn2eAKA7hMEa\" width=75% title=\"Bamboo Forest, src: Adobe Stock\"></img>\n",
        "</center>"
      ],
      "metadata": {
        "id": "CVV9jpSw-UZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### First attempt"
      ],
      "metadata": {
        "id": "t2GKipow-XV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a RandomForestRegressor with default parameters\n",
        "naive_rf_model = RandomForestRegressor()\n",
        "\n",
        "# Fit the model using the normalized training set\n",
        "naive_rf_model.fit(X_train_normalized, Y_train_normalized)"
      ],
      "metadata": {
        "id": "hSSYmfbWEn8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Score the naive Random Forest model\n",
        "score_model(naive_rf_model,\n",
        "            X_train_normalized, Y_train_normalized,\n",
        "            X_val_normalized, Y_val_normalized,\n",
        "            X_test_normalized, Y_test_normalized,\n",
        "            model_name=\"Naive Random Forest\")\n",
        "\n",
        "print(\"Accumulated Model Scores:\")\n",
        "print_model_scores(model_scores)"
      ],
      "metadata": {
        "id": "7LRxMOo3E4Tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hyperparameter sweep"
      ],
      "metadata": {
        "id": "Vxvjozry_Hqg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To set up a hyperparameter sweep and data pipeline for a Random Forest model, we will follow these steps:\n",
        "\n",
        "1. Data Preparation: Use the normalized training set (X_train_normalized, Y_train_normalized).\n",
        "\n",
        "2. Model and Hyperparameters:\n",
        "\n",
        "* Define the Random Forest model and set its parameters: max_depth, n_estimators, max_features, min_samples_leaf.\n",
        "* Specify the scoring function (r2 for RÂ² score).\n",
        "* Use the 'squared_error' criterion for the Random Forest.\n",
        "\n",
        "3. Cross-Validation: Set up cross-validation parameters using KFold.\n",
        "\n",
        "4. Model Fitting: Fit the Random Forest model to X_train_normalized and Y_train_normalized.\n",
        "\n",
        "Random Forest Model Definitions\n",
        "\n",
        "* criterion: Use 'squared_error' for mean squared error.\n",
        "* max_features: For regression, typically use N_features, but other values like sqrt(N_features) or N_features/3 are also common. Options: [5, 6, 15, 17].\n",
        "* n_estimators: Number of decision trees. Options: [100, 250].\n",
        "* min_samples_leaf: Minimum number of samples required to create a leaf node. Options: [2, 4, 8].\n",
        "* max_depth: Maximum depth of each decision tree. Options: [5, 6, 8].\n",
        "\n",
        "Cross-Validation Definitions\n",
        "\n",
        "* k_folds: Number of folds for cross-validation. We will use 10 folds."
      ],
      "metadata": {
        "id": "3sE6gDa1_x_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_gridsearch_RF_regressor(score, max_depth, n_estimators, max_features,\n",
        "                                   min_samples_leaf, cv, scoring_metric='r2'):\n",
        "    \"\"\"\n",
        "    Create a GridSearchCV object for a RandomForestRegressor.\n",
        "\n",
        "    Parameters:\n",
        "    - score: list of scoring functions to use (e.g., ['squared_error'])\n",
        "    - max_depth: list of max_depth values to try\n",
        "    - n_estimators: list of n_estimators values to try\n",
        "    - max_features: list of max_features values to try\n",
        "    - min_samples_leaf: list of min_samples_leaf values to try\n",
        "    - cv: cross-validation splitting strategy (e.g., PredefinedSplit object)\n",
        "    - scoring_metric: scoring metric to use (default is 'r2')\n",
        "\n",
        "    Returns:\n",
        "    - rfgs: GridSearchCV object\n",
        "    \"\"\"\n",
        "    rfmodel = RandomForestRegressor()\n",
        "    params = {\n",
        "        'max_depth': max_depth,\n",
        "        'criterion': score,\n",
        "        'max_features': max_features,\n",
        "        'n_estimators': n_estimators,\n",
        "        'min_samples_leaf': min_samples_leaf\n",
        "    }\n",
        "    rfgs = GridSearchCV(rfmodel, param_grid=params, cv=cv, scoring=scoring_metric)\n",
        "    return rfgs"
      ],
      "metadata": {
        "id": "UYi8Er0Z_VNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter options\n",
        "# This may be a bit long depending on how many HPs you decide to try out\n",
        "# Proceed with caution\n",
        "k_folds = 10\n",
        "max_depth = [5, 6, 8]\n",
        "score = ['squared_error']\n",
        "max_features = [5, 6, 15, 17]\n",
        "n_estimators = [100, 250]\n",
        "min_samples_leaf = [2, 4, 8]"
      ],
      "metadata": {
        "id": "EgIuImxcAYk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To perform validation on the pre-defined validation set,\n",
        "# We create a PredefinedSplit object\n",
        "# `-1` indicates the training set, and any other value indicates the validation set.\n",
        "trval_fold = np.concatenate([\n",
        "    np.full(X_train_normalized.shape[0], -1),\n",
        "    np.zeros(X_val_normalized.shape[0])\n",
        "])\n",
        "\n",
        "# Combine the training and validation sets\n",
        "X_combined = np.concatenate([X_train_normalized, X_val_normalized])\n",
        "Y_combined = np.concatenate([Y_train_normalized, Y_val_normalized])\n",
        "\n",
        "ps = PredefinedSplit(test_fold=trval_fold)\n",
        "\n",
        "# Create GridSearchCV object\n",
        "rfgs = create_gridsearch_RF_regressor(score, max_depth, n_estimators,\n",
        "                                      max_features, min_samples_leaf, ps)\n",
        "\n",
        "# Fit the model using the combined training and validation sets\n",
        "rfgs.fit(X_combined, Y_combined)"
      ],
      "metadata": {
        "id": "Uo44n3o2HonY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the best model and parameters\n",
        "best_model = rfgs.best_estimator_\n",
        "best_params = rfgs.best_params_\n",
        "\n",
        "print(\"Best Model Parameters:\", best_params)"
      ],
      "metadata": {
        "id": "Av7bHXLcIlWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you don't have time to optimize hyperparameters, we found the following best model parameters:\n",
        "\n",
        "Best Model Parameters: `{'criterion': 'squared_error', 'max_depth': 5, 'max_features': 5, 'min_samples_leaf': 8, 'n_estimators': 100}`"
      ],
      "metadata": {
        "id": "BJflGiP-D-bv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate and print scores for the best model, including the test set\n",
        "score_model(best_model,\n",
        "            X_train_normalized, Y_train_normalized,\n",
        "            X_val_normalized, Y_val_normalized,\n",
        "            X_test_normalized, Y_test_normalized,\n",
        "            model_name=\"Best Random Forest\")"
      ],
      "metadata": {
        "id": "ln3K72ivAcyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_model_scores(model_scores)"
      ],
      "metadata": {
        "id": "Tq7xTN-wAr7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The naive random forest is clearly overfitting! Let's cut down the number of features using explainable artificial intelligence methods ðŸ§ "
      ],
      "metadata": {
        "id": "s4gLZVL0EZit"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Gini-impurity based feature selection"
      ],
      "metadata": {
        "id": "9P5RaKerEhSj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gini Impurity**:\n",
        "- Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the set.\n",
        "- It is used by the CART (Classification and Regression Tree) algorithm for classification problems.\n",
        "- The formula for the Gini impurity can be found on [Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning#:~:text=Gini%20impurity%20measures%20how%20often,into%20a%20single%20target%20category.)\n",
        "\n",
        "**Feature Importance Using Gini Impurity**:\n",
        "- Feature importance in the context of decision trees and random forests refers to the contribution of each feature to the prediction accuracy of the model.\n",
        "- Random forests calculate feature importance by looking at the decrease in Gini impurity caused by a feature.\n",
        "- For each feature, the random forest algorithm computes the total reduction in Gini impurity brought by that feature across all trees in the forest. The average of these reductions is used as the importance score for the feature."
      ],
      "metadata": {
        "id": "DYO17kUjI1mC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Feature Importance for the best RF model\n",
        "feature_importances = best_model.feature_importances_"
      ],
      "metadata": {
        "id": "V64YSC2vKEk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the feature importances and label them using X_metadata\n",
        "# Using horizontal bars, from top (the most important feature)\n",
        "# to bottom (the least important feature)\n",
        "# Sort features by importance\n",
        "sorted_indices = np.argsort(feature_importances)[::-1]\n",
        "sorted_features = [feature_names[i] for i in sorted_indices]\n",
        "sorted_importances = feature_importances[sorted_indices]\n",
        "\n",
        "# Plot the feature importances\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.barh(range(len(sorted_importances)), sorted_importances, align='center')\n",
        "plt.yticks(range(len(sorted_importances)), sorted_features)\n",
        "plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature at the top\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.title('Feature Importances Using Gini Impurity')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2BRD6C4tKXZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clearly the top 4 features stand out! Let's retrain the model accordingly."
      ],
      "metadata": {
        "id": "8GMEA71oLBxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select Top 4 Features\n",
        "top_4_features_indices = np.argsort(feature_importances)[-4:]\n",
        "\n",
        "# Filter training and validation sets to keep only the top 4 features\n",
        "X_train_top_4 = X_train_normalized[:, top_4_features_indices]\n",
        "X_val_top_4 = X_val_normalized[:, top_4_features_indices]\n",
        "\n",
        "trval_fold = np.concatenate([\n",
        "    np.full(X_train_top_4.shape[0], -1),\n",
        "    np.zeros(X_val_top_4.shape[0])\n",
        "])\n",
        "\n",
        "# Combine the training and validation sets\n",
        "X_combined = np.concatenate([X_train_top_4, X_val_top_4])\n",
        "Y_combined = np.concatenate([Y_train_normalized, Y_val_normalized])\n",
        "\n",
        "ps = PredefinedSplit(test_fold=trval_fold)\n",
        "\n",
        "# Create GridSearchCV object\n",
        "rfgs_top_4 = create_gridsearch_RF_regressor(score, max_depth, n_estimators,\n",
        "                                      max_features, min_samples_leaf, ps)"
      ],
      "metadata": {
        "id": "KIvV_6gxLFnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model using the training set with top 4 features\n",
        "rfgs_top_4.fit(X_combined, Y_combined)\n",
        "\n",
        "# Get the best model and parameters\n",
        "best_model_top_4 = rfgs_top_4.best_estimator_\n",
        "best_params_top_4 = rfgs_top_4.best_params_\n",
        "\n",
        "print(\"Best Model Parameters for Top 4 Features:\", best_params_top_4)"
      ],
      "metadata": {
        "id": "E4R1Xo-cLOkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Score the model using the validation set with top 4 features\n",
        "score_model(best_model_top_4,\n",
        "            X_train_top_4, Y_train_normalized,\n",
        "            X_val_top_4, Y_val_normalized,\n",
        "            X_test_normalized[:, top_4_features_indices], Y_test_normalized,\n",
        "            model_name=\"RF Top 4 features\")"
      ],
      "metadata": {
        "id": "yXZLw_11LQ6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_model_scores(model_scores)"
      ],
      "metadata": {
        "id": "g98kH0tgLd96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is still disappointing. Can XGBoost do better?"
      ],
      "metadata": {
        "id": "HiWfkrjANBO6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### XGBoost"
      ],
      "metadata": {
        "id": "3H_xJPZK_Jnw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**XGBoost Advantages:**\n",
        "\n",
        "1. **Boosting Technique**: XGBoost uses boosting to build models that focus on correcting errors from previous ones, resulting in a strong, accurate ensemble model.\n",
        "\n",
        "2. **Regularization**: It includes L1 and L2 regularization to prevent overfitting and improve generalization, especially with complex datasets.\n",
        "\n",
        "3. **Handling Missing Values**: XGBoost can handle missing values during training, making it suitable for real-world datasets.\n",
        "\n",
        "4. **Parallel Processing**: XGBoost uses parallel processing, making it faster and more efficient, especially with large datasets.\n",
        "\n",
        "**Advantages of Random Hyperparameter Search (over e.g., grid search):**\n",
        "\n",
        "1. **Efficiency**: Random search explores a broader range of hyperparameters by sampling randomly, often finding better configurations with fewer trials.\n",
        "\n",
        "2. **Scalability**: It can be easily parallelized to test multiple hyperparameter combinations at once, making it faster than grid search.\n",
        "\n",
        "3. **Continuous Hyperparameters**: Random search handles continuous hyperparameters more effectively by sampling from a continuous distribution, increasing the chance of finding optimal values."
      ],
      "metadata": {
        "id": "aMsa77kdQURN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_randomsearch_XGB_regressor(learning_rate, max_depth, n_estimators,\n",
        "                                      colsample_bytree, min_child_weight, cv,\n",
        "                                      scoring_metric='r2', n_iter=100):\n",
        "    \"\"\"\n",
        "    Create a RandomizedSearchCV object for an XGBRegressor.\n",
        "\n",
        "    Parameters:\n",
        "    - learning_rate: list of learning rate values to try\n",
        "    - max_depth: list of max_depth values to try\n",
        "    - n_estimators: list of n_estimators values to try\n",
        "    - colsample_bytree: list of colsample_bytree values to try\n",
        "    - min_child_weight: list of min_child_weight values to try\n",
        "    - cv: cross-validation splitting strategy (e.g., PredefinedSplit object)\n",
        "    - scoring_metric: scoring metric to use (default is 'r2')\n",
        "    - n_iter: number of parameter settings that are sampled (default is 100)\n",
        "\n",
        "    Returns:\n",
        "    - xgb_rs: RandomizedSearchCV object\n",
        "    \"\"\"\n",
        "    xgb_model = xgb.XGBRegressor(objective='reg:squarederror')\n",
        "    param_distributions = {\n",
        "        'learning_rate': learning_rate,\n",
        "        'max_depth': max_depth,\n",
        "        'n_estimators': n_estimators,\n",
        "        'colsample_bytree': colsample_bytree,\n",
        "        'min_child_weight': min_child_weight\n",
        "    }\n",
        "    xgb_rs = RandomizedSearchCV(xgb_model, param_distributions=param_distributions,\n",
        "                                cv=cv, scoring=scoring_metric, n_iter=n_iter, random_state=42)\n",
        "    return xgb_rs"
      ],
      "metadata": {
        "id": "nD-nQqAMMyLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter options for XGBoost\n",
        "\n",
        "# Learning rate (also known as eta) controls the step size at each iteration while moving toward a minimum of the loss function.\n",
        "learning_rate = [0.01, 0.05, 0.1, 0.2, 0.3]  # Try different learning rates\n",
        "\n",
        "# Max depth determines the maximum depth of the trees.\n",
        "# Deeper trees can model more complex patterns but may overfit.\n",
        "max_depth = [3, 5, 7, 9, 12]  # Try different maximum depths for the trees\n",
        "\n",
        "# Number of estimators is the number of trees in the model (ensemble size).\n",
        "# More trees can improve performance but also increase computation time.\n",
        "n_estimators = [50, 100, 200, 300]  # Try different numbers of trees\n",
        "\n",
        "# Colsample_bytree is the subsample ratio of columns when constructing each tree.\n",
        "# It is used to prevent overfitting by sampling a fraction of features.\n",
        "colsample_bytree = [0.3, 0.5, 0.7, 0.9, 1.0]  # Try different subsample ratios for columns\n",
        "\n",
        "# Min child weight is the minimum sum of instance weight (Hessian) needed in a child. It is used to control overfitting.\n",
        "min_child_weight = [1, 3, 5, 7]  # Try different minimum child weights\n",
        "\n",
        "# Create a PredefinedSplit object\n",
        "# `-1` indicates the training set, and `0` indicates the validation set.\n",
        "test_fold = np.concatenate([\n",
        "    np.full(X_train_normalized.shape[0], -1),\n",
        "    np.zeros(X_val_normalized.shape[0])\n",
        "])\n",
        "\n",
        "# Combine the training and validation sets\n",
        "X_combined = np.concatenate([X_train_normalized, X_val_normalized])\n",
        "Y_combined = np.concatenate([Y_train_normalized, Y_val_normalized])\n",
        "\n",
        "ps = PredefinedSplit(test_fold=test_fold)\n",
        "\n",
        "# Create RandomizedSearchCV object for XGBoost\n",
        "xgb_rs = create_randomsearch_XGB_regressor(learning_rate, max_depth, n_estimators,\n",
        "                                           colsample_bytree, min_child_weight, ps)"
      ],
      "metadata": {
        "id": "iyYU1CshNfJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model using the combined training and validation sets\n",
        "xgb_rs.fit(X_combined, Y_combined)\n",
        "\n",
        "# Get the best model and parameters\n",
        "best_xgb_model = xgb_rs.best_estimator_\n",
        "best_xgb_params = xgb_rs.best_params_\n",
        "\n",
        "print(\"Best Model Parameters for XGBoost:\", best_xgb_params)"
      ],
      "metadata": {
        "id": "UrTCEGmJOI4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show feature importance\n",
        "xgb.plot_importance(best_xgb_model)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bEXvTaoKUW7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Score the model using the validation set with all features\n",
        "score_model(best_xgb_model,\n",
        "            X_train_normalized, Y_train_normalized,\n",
        "            X_val_normalized, Y_val_normalized,\n",
        "            X_test_normalized, Y_test_normalized,\n",
        "            model_name=\"XGBoost All features\")"
      ],
      "metadata": {
        "id": "OnXewgcTOTxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_model_scores(model_scores)"
      ],
      "metadata": {
        "id": "_0-OsT_tOV32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now it's your turn: Create your own model ðŸ–Œ\n",
        "ðŸŽ¨ Be creative ðŸŽµ\n",
        "\n",
        "You may consider polynomial models, symbolic regression, other types of neural networks, etc.\n",
        "\n",
        "Given that some variables are clearly more informative, you could use memory and more timesteps to leverage finer temporal information.\n",
        "\n",
        "Of course, if you use more information, be mindful to regularize your model, e.g., using L1/L2 regularization. You may for instance use an [Elastic Net](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html)."
      ],
      "metadata": {
        "id": "mQ6x3Dp3mi_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create your model here"
      ],
      "metadata": {
        "id": "9fW7BUeLPvqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Don't forget to score it\n",
        "?score_model"
      ],
      "metadata": {
        "id": "Nf_f67-MPxLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Place all models on a Pareto front"
      ],
      "metadata": {
        "id": "uCbozYK0mpG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pareto fronts allow to easily compare two competing objectives, such as complexity (e.g., the number of trainable parameters) and error"
      ],
      "metadata": {
        "id": "phxWeoyLOpUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to determine Pareto optimal points\n",
        "\n",
        "def is_pareto_efficient_simple(costs):\n",
        "\n",
        "    \"\"\"\n",
        "    Identify the Pareto-efficient points for a given set of costs.\n",
        "\n",
        "    Parameters:\n",
        "    costs (np.array): A 2D NumPy array where each row represents an individual point\n",
        "                      in terms of its cost dimensions. For instance, each row could\n",
        "                      represent a different configuration's cost as [cost1, cost2].\n",
        "\n",
        "    Returns:\n",
        "    np.array: A boolean array where True indicates that the corresponding point in\n",
        "              'costs' is Pareto efficient.\n",
        "\n",
        "    Description:\n",
        "    This function iterates over each point and determines whether it is dominated\n",
        "    by any other point. A point is considered Pareto efficient if no other point\n",
        "    exists that is better in all cost dimensions. In this context, \"better\" is\n",
        "    defined as being lower for each cost dimension since we are minimizing.\n",
        "\n",
        "    The function uses a boolean array, `is_efficient`, to keep track of which\n",
        "    points are currently considered Pareto efficient. Initially, all points are\n",
        "    assumed to be Pareto efficient.\n",
        "\n",
        "    For each point:\n",
        "    1. If the point is still considered potentially Pareto efficient (`is_efficient[i]` is True),\n",
        "       it checks against all other points that are also still marked as efficient.\n",
        "    2. It updates the `is_efficient` array by setting it to False for any point\n",
        "       that is dominated by the current point. A point `j` is dominated by point `i`\n",
        "       if all its cost dimensions are greater than or equal to those of point `i`\n",
        "       and at least one dimension is strictly greater.\n",
        "    3. It ensures the current point's efficiency status remains True regardless of the\n",
        "       comparison outcome by resetting `is_efficient[i]` to True after the comparison.\n",
        "\n",
        "    The loop ensures that every point is compared with all others, and after processing,\n",
        "    the `is_efficient` array flags only those points that are not dominated by any other,\n",
        "    indicating the Pareto front.\n",
        "    \"\"\"\n",
        "\n",
        "    is_efficient = np.ones(costs.shape[0], dtype=bool)\n",
        "    for i, c in enumerate(costs):\n",
        "        if is_efficient[i]:\n",
        "            is_efficient[is_efficient] = np.any(costs[is_efficient] < c, axis=1)\n",
        "            is_efficient[i] = True\n",
        "    return is_efficient"
      ],
      "metadata": {
        "id": "WBUjG8IMSLWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting data into a DataFrame\n",
        "data = []\n",
        "for model, scores in model_scores.items():\n",
        "    test_scores = scores['test']\n",
        "    num_params = scores['num_params']\n",
        "    data.append([\n",
        "        model,\n",
        "        test_scores[\"RMSE (knots)\"],\n",
        "        test_scores[\"RMSE (hPa)\"],\n",
        "        num_params,\n",
        "        np.log10(num_params)\n",
        "    ])\n",
        "\n",
        "df = pd.DataFrame(data, columns=[\"Model\", \"Test_RMSE_knots\", \"Test_RMSE_hPa\", \"Number of Parameters\", \"log10_Number of Parameters\"])\n",
        "\n",
        "# Define the cost matrix with log10(Number of Parameters) and Test RMSE for hPa\n",
        "costs_hPa = df[['log10_Number of Parameters', 'Test_RMSE_hPa']].values\n",
        "\n",
        "# Find Pareto efficient points for RMSE (hPa)\n",
        "pareto_efficient_hPa = is_pareto_efficient_simple(costs_hPa)\n",
        "pareto_models_hPa = df[pareto_efficient_hPa].sort_values(by=['log10_Number of Parameters'])\n",
        "\n",
        "# Define the cost matrix with log10(Number of Parameters) and Test RMSE for knots\n",
        "costs_knots = df[['log10_Number of Parameters', 'Test_RMSE_knots']].values\n",
        "\n",
        "# Find Pareto efficient points for RMSE (knots)\n",
        "pareto_efficient_knots = is_pareto_efficient_simple(costs_knots)\n",
        "pareto_models_knots = df[pareto_efficient_knots].sort_values(by=['log10_Number of Parameters'])\n",
        "\n",
        "# Display the Pareto optimal models\n",
        "print(\"Pareto Optimal Models for RMSE (hPa):\")\n",
        "print(pareto_models_hPa)\n",
        "print(\"\\nPareto Optimal Models for RMSE (knots):\")\n",
        "print(pareto_models_knots)"
      ],
      "metadata": {
        "id": "6KL4sv2ETZwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df, pareto_models_hPa, and pareto_models_knots are defined\n",
        "\n",
        "# Plot setup\n",
        "fig, ax = plt.subplots(2, 1, figsize=(11.5, 9.5))\n",
        "\n",
        "# Plot for RMSE (hPa)\n",
        "colors = plt.cm.get_cmap('tab10', len(df))\n",
        "texts = []\n",
        "for i, row in df.iterrows():\n",
        "    ax[0].scatter(row['log10_Number of Parameters'],\n",
        "                  row['Test_RMSE_hPa'], color=colors(i), label=row['Model'])\n",
        "    texts.append(ax[0].text(row['log10_Number of Parameters'],\n",
        "                            row['Test_RMSE_hPa']+.01, row['Model'], fontsize=9))\n",
        "\n",
        "ax[0].set_xlabel('log10(Number of Parameters)')\n",
        "ax[0].set_ylabel('Test RMSE (hPa)')\n",
        "ax[0].set_title('Pareto Front for MSLP (hPa)')\n",
        "\n",
        "pareto_params_hPa = pareto_models_hPa['log10_Number of Parameters'].values\n",
        "pareto_rmse_hPa = pareto_models_hPa['Test_RMSE_hPa'].values\n",
        "\n",
        "# Draw the Pareto front for RMSE (hPa)\n",
        "for i in range(len(pareto_params_hPa)):\n",
        "    if i > 0:\n",
        "        ax[0].vlines(x=pareto_params_hPa[i], ymin=pareto_rmse_hPa[i-1],\n",
        "                     ymax=pareto_rmse_hPa[i], color='k', linewidth=1, zorder=-1)\n",
        "    ax[0].hlines(y=pareto_rmse_hPa[i],\n",
        "                 xmin=pareto_params_hPa[i],\n",
        "                 xmax=pareto_params_hPa[i+1] if i+1 < len(pareto_params_hPa) \\\n",
        "                 else max(df['log10_Number of Parameters']),\n",
        "                 color='k', linewidth=1, zorder=-1)\n",
        "\n",
        "# Adjust text positions to avoid overlap\n",
        "adjust_text(texts, ax=ax[0], only_move={'points':'y', 'texts':'y'},\n",
        "            arrowprops=dict(arrowstyle='-', color='grey'))\n",
        "\n",
        "# Plot for RMSE (knots)\n",
        "texts = []\n",
        "for i, row in df.iterrows():\n",
        "    ax[1].scatter(row['log10_Number of Parameters'], row['Test_RMSE_knots'],\n",
        "                  color=colors(i), label=row['Model'])\n",
        "    texts.append(ax[1].text(row['log10_Number of Parameters'], row['Test_RMSE_knots']+.01,\n",
        "                            row['Model'], fontsize=9))\n",
        "\n",
        "ax[1].set_xlabel('log10(Number of Parameters)')\n",
        "ax[1].set_ylabel('Test RMSE (knots)')\n",
        "ax[1].set_title('Pareto Front for Intensity (knots)')\n",
        "\n",
        "pareto_params_knots = pareto_models_knots['log10_Number of Parameters'].values\n",
        "pareto_rmse_knots = pareto_models_knots['Test_RMSE_knots'].values\n",
        "\n",
        "# Draw the Pareto front for RMSE (knots)\n",
        "for i in range(len(pareto_params_knots)):\n",
        "    if i > 0:\n",
        "        ax[1].vlines(x=pareto_params_knots[i], ymin=pareto_rmse_knots[i-1],\n",
        "                     ymax=pareto_rmse_knots[i],\n",
        "                     color='k', linewidth=1, zorder=-1)\n",
        "    ax[1].hlines(y=pareto_rmse_knots[i], xmin=pareto_params_knots[i],\n",
        "                 xmax=pareto_params_knots[i+1] if i+1 < len(pareto_params_knots) \\\n",
        "                 else max(df['log10_Number of Parameters']),\n",
        "                 color='k', linewidth=1, zorder=-1)\n",
        "\n",
        "# Adjust text positions to avoid overlap\n",
        "adjust_text(texts, ax=ax[1], only_move={'points':'y', 'texts':'y'},\n",
        "            arrowprops=dict(arrowstyle='-', color='grey'))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mpfSCBoTT2as"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}